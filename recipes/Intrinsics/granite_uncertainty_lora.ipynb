{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty LoRA with Granite Uncertainty 3.0 8b\n",
    "*Using IBM Granite Models*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains instructions on how to leverage a LoRA adapter for IBM's Granite model. This specific adapter is designed to provide a calibrated certainty score when answering questions that are prompted, while still retaining the full abilities of the original granite-3.0-8b-instruct model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few use cases where having the certainty score would be helpful:\n",
    "- Human usage: Certainty scores give human users an indication of when to trust answers from the model (which should be augmented by their own knowledge).\n",
    "- Model routing/guards: If the model has low certainty (below a chosen threshold), it may be worth sending the request to a larger, more capable model or simply choosing not to show the response to the user.\n",
    "- RAG: Granite Uncertainty 3.0 8b is calibrated on document-based question answering datasets, hence it can be applied to giving certainty scores for answers created using RAG. This certainty will be a prediction of overall correctness based on both the documents given and the model's own knowledge (e.g. if the model is correct but the answer is not in the documents, the certainty can still be high)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will walk through how to set up and test the LoRA adapters capabilities so that you will be able to apply it to your own use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up your environment\n",
    "\n",
    "First ensure you are running python 3.10 or 3.11 in a freshly-created virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 10) and sys.version_info < (3, 12), \"Use Python 3.10 or 3.11 to run this notebook.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies\n",
    "\n",
    "First we'll install some dependencies. Granite utils comes with some helpful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install \"git+https://github.com/ibm-granite-community/utils.git\" \\\n",
    "    transformers \\\n",
    "    peft \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup your model and adapter\n",
    "\n",
    "Next we will create a model object for your Granite Uncertainty model. This can take quite a bit of memory (>16 GB). To do that let's setup the backend and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft.peft_model import PeftModel\n",
    "import torch, os\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('ibm-granite/granite-3.0-8b-instruct', padding_side='left', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we will set up each of the models, and create the merged model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base = AutoModelForCausalLM.from_pretrained('ibm-granite/granite-3.0-8b-instruct')\n",
    "model_lora = PeftModel.from_pretrained(model_base, 'ibm-granite/granite-uncertainty-3.0-8b-lora')\n",
    "model = model_lora.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your prompt\n",
    "\n",
    "Next, let's set up your system prompt and user prompt. The granite model was calibrated using a specific system prompt, which we have stored as \"system_prompt\" below, which we then merge with the user \"question\" to form the \"question_chat\" prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an AI language model developed by IBM Research. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior.\" \n",
    "question = input(\"Please enter your question: \")\n",
    "print(\"Question:\" + question)\n",
    "question_chat = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_prompt\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": question\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = tokenizer.apply_chat_template(question_chat,tokenize=False,add_generation_prompt=True)\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "output = model_lora.generate(inputs[\"input_ids\"].to(device), attention_mask=inputs[\"attention_mask\"].to(device), max_new_tokens=80)\n",
    "output_text = tokenizer.decode(output[0])\n",
    "answer = output_text.split(\"assistant<|end_of_role|>\")[1]\n",
    "print(\"Answer: \" + answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating certainty score\n",
    "Once we have generated our answer, we will run it back through the LoRA tuned adapter to get an uncertainty score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uq_generation_prompt = \"<|start_of_role|>certainty<|end_of_role|>\"\n",
    "uq_chat = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_prompt\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": question\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": answer\n",
    "    },\n",
    "]\n",
    "\n",
    "uq_text = tokenizer.apply_chat_template(uq_chat,tokenize=False) + uq_generation_prompt\n",
    "inputs = tokenizer(uq_text, return_tensors=\"pt\")\n",
    "output = model.generate(inputs[\"input_ids\"].to(device), attention_mask=inputs[\"attention_mask\"].to(device), max_new_tokens=1)\n",
    "output_text = tokenizer.decode(output[0])\n",
    "uq_score = int(output_text[-1])\n",
    "print(\"Certainty: \" + str(5 + uq_score * 10) + \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
